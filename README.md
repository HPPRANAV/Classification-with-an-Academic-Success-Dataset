This repository contains the code and associated training and testing data for the "Classification with an Academic Success" competition.

Overview
This competition aims to classify academic success using various machine learning models. The models utilized in this project include:

Gradient-Boosted Decision Trees
Support Vector Machines (SVM)
XGBoost
Random Forest
Multi-Layer Perceptron (MLP)
Feature Engineering
Explicit feature engineering has been performed, where a combination of arithmetic operations is used to generate new features. Additionally, domain-specific features relevant to academic success are included to further improve model performance.

Models
Gradient-Boosted Decision Trees: An ensemble learning technique that builds multiple decision trees and combines their outputs to improve accuracy and prevent overfitting.

Support Vector Machines (SVM): A supervised learning model that finds the hyperplane that best separates the data into different classes.

XGBoost: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.

Random Forest: An ensemble method that constructs multiple decision trees during training and outputs the mode of the classes for classification tasks.

Multi-Layer Perceptron (MLP): A class of feedforward artificial neural network that consists of at least three layers of nodes, capable of modeling complex relationships in data.

Data
The dataset used in this project consists of training and testing data specifically prepared for the competition. The data includes various features related to academic performance, which are used to predict academic success.

Performance
The implemented models yield an accuracy of 82%.
